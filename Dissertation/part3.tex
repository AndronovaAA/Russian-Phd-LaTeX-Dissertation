\chapter{Предлагаемый метод}\label{ch:ch3}
\section{Математические инструменты}\label{sec:ch3/sect1}
В данном разделе мы обобщим несколько результатов, которые будут использованы в последующих разделах работы.
\begin{lemma}\label{lemma:Young}
	(Неравенство Юнга).
	Существует множество различных версий неравенства Юнга, которые используются для работы с линейно матричными неравенствами. Все они могут быть выведены друг из друга. В данной работе мы будем использовать следующие две версии:
	%
	Для любой положительно определённой матрицы $M>0$, положительных скаляров $\epsilon > 0, \nu > 0$ и матриц ${X}, {Y}, {F}$, где ${F}\T{F}\leq \nu{I}$, следующие неравенства верны \cite{BOYED1994}:
	%
	\begin{align}
		\label{eq:Young_relation_BMI}
		{X}\T{Y} + {Y}\T{X}  \leq {X}\T 
		\epsilon {M}^{-1}{X} + \frac{1}{\epsilon}   {Y}\T  {M}{Y}, 
		\\
		\label{eq:Young_robust_2}
		{X}\T{F}{Y} + {Y}\T{F}\T{X}  \leq \epsilon {X}\T{X} +  \frac{\nu}{\epsilon} {Y}\T {Y}.
	\end{align}
	%
	следуя \cite{LIEN2008} мы можем выбрать $\nu=\epsilon^2$ для формулирования следующего неравенства:
	%
	\begin{equation}
		\label{eq:updated_Young_robust_2}
		{X}^T{F}{Y} + {Y}\T{F}\T{X}  \leq \epsilon {X}\T{X} + \epsilon {Y}\T{Y}.
	\end{equation}
\end{lemma}

\begin{lemma}\label{lemma:S_procedure}
	(S-Процедура).
	Рассмотрим ${M},{N} \in \mathbb{R}^{n\times n}$. Если существует скаляр $\gamma>0$, который отвечает следующему условию ${M}+\gamma {N}<0$, тогда $x\T {N} x\geq 0$ подразумевает $x\T{M}x\leq 0$ \cite{BOYED1994}.
\end{lemma}

\begin{lemma}\label{lemma:Schur}
	(Дополнение Шура \cite{Schur}).
	Для любой симметричной матрицы ${A}\in \mathbb{S}^n$ и ${C}\in \mathbb{S}^m$ и матрицы ${B}\in \mathbb{R}^{n\times m}$, а также их конкатенации:
	\noindent \begin{align*}
		{M}= \begin{bmatrix}
			{A} & {B} \\
			{B}\T & {C} 
		\end{bmatrix},
	\end{align*}
	%
	Следующие утверждения равнозначны:
	% 
	\noindent
	\begin{enumerate}
		\item ${M} < 0$,
		\item ${A}-{B}{C}^{-1}{B}\T < 0 , {C}< 0$.
		\item ${C}-{B}\T{A}^{-1}{B}< 0 , {A}< 0$
	\end{enumerate}
\end{lemma}

\section{Мультипликативная неопределённость}\label{sec:ch3/sect2}
В этом разделе мы рассматриваем формулировку, в которой мультипликативные неопределённости накладываются на каждую из матриц модели. Мы показываем, что эта задача может быть решена как в случае строго ограниченной неопределённости, так и в случае мягко ограниченной неопределённости. Наша цель - предоставить новые методы для проектирования робастного управления, а также показать дополнительную устойчивость к неопределённости, которая может быть достигнута при отмене статического влияния состояния.

Рассмотрим следующую систему: 
%
\begin{equation}
	\label{eq:part2_linear_dynamics}
	\begin{cases}
		\dot z=({A}_n+\Delta {A}_n)z + ({B}+\Delta {B})u,\\
		y = ({C}+ \Delta {C}){N}  z,
	\end{cases}
\end{equation}
%
где неопределённые матрицы состояния, управления и наблюдения определяются как:
%
\begin{equation}
	\label{eq:part2_uncertainty}
	\Delta {A}_n={M}_1{F}_1{N}_1, \ \ \Delta {B}= {M}_2{F}_2{N}_2, \ \
	\Delta {C} = {M}_3{F}_3{N}_3; 
\end{equation}
%
где ${M}_1 \in \mathbb{R}^{n_z \times d}$, 
${N}_1 \in \mathbb{R}^{d \times n_z}$ , ${M}_2 \in \mathbb{R}^{n_z \times p}$,
${N}_2 \in \mathbb{R}^{p \times m}$, ${M}_3 \in \mathbb{R}^{l \times q}$,
${N}_3 \in \mathbb{R}^{q \times n}$ - известные матрицы, а ${F}_i$ - неизвестные матрицы ограниченные по норме ${F}_i\T{F}_i\leq \nu_i {I}$, где $\nu_i$ - радиусы неопределённости - скаляры, определяющие ограничения, накладываемые на нормы ${F}_i$. Мы вводим наблюдателя Люенбергера с состоянием $\hat{z}$:
%
\begin{equation}
	\hat{z}={A}_n\hat{z}+{B}u+{L}_z(y- {C} {N}\hat{z}).
\end{equation}
%
Учитывая ошибку оценки $e_z=z-\hat{z}$, мы можем записать динамику ошибки наблюдателя:
%
\begin{equation}
	\label{eq:part2_error_dynamics}
	\dot{e}_z=({A}_n-{L}_z{C}{N}) e_z +(\Delta {A}_n -{L}_z\Delta {C}{N}) z +\Delta {B} u.
\end{equation}
%
Рассмотрим следующий закон управления с линейной обратной связью:
%
\begin{equation}
	u={K}_z\hat{z}.
\end{equation}
%
При этом динамика замкнутого цикла принимает вид:
%
\begin{equation}
	\label{eq:part2_system}
	\begin{bmatrix}
		\dot{z} \\ \dot{e}_z
	\end{bmatrix}=\begin{bmatrix}
		({A}_n+\Delta {A}_n +{B}{K}_z+\Delta {B}{K}_z) & -({B}{K}_z+\Delta {B}{K}_z) \\
		(\Delta {A}_n +\Delta {B}{K}_z-{L}_z\Delta {C}{N}) & ({A}_n-{L}_z{C}{N}-\Delta {B}{K}_z)        \end{bmatrix}\begin{bmatrix}
		z \\ e_z
	\end{bmatrix}.
\end{equation}
%
\subsection{Единичная неопределённость}\label{sec:ch3/sect2/sub1}
В этом подразделе мы рассмотрим случай, когда радиус неопределённости равен $\nu_i = 1$, поэтому границы неопределённости определяются как:
%
\begin{equation}
	\label{eq:part2_one_uncertainty}
	{F}_i\T{F}_i\leq {I}, \ \ i=1,2,3;
\end{equation}
%
\begin{theorem}\label{thm:part2_LMI_1}
	Система \eqref{eq:part2_system} асимптотически устойчива при любом выборе $\Delta {A}_n$, $\Delta {B}$, $\Delta {C}$, удовлетворяющих условиям \eqref{eq:part2_uncertainty} и \eqref{eq:part2_one_uncertainty}, если существуют положительно-определённые матрицы ${Q}_1>0$, ${P}_2>0$, матрицы $\hat{{K}}$, $\hat{{L}}$,
	и положительные скаляры $\alpha_1>0,\beta_1>0,\alpha_2>0,\beta_2>0,\alpha_3>0,\beta_3>0$ и $\epsilon_1 > 0$ такие, что выполняется следующее линейно-матричное неравенство:
	%
	\begin{equation}
		\label{eq:thm3_final_LMI}
		\begin{bmatrix}
			\mathcal{T}_{11} & \mathcal{T}_{12} \\
			* & \mathcal{T}_{22}
		\end{bmatrix}<0,
	\end{equation}
	%
	где
	%
	\begin{equation}
		\mathcal{T}_{11}= \begin{bmatrix}
			{\Lambda}_1 & 0 & {M}_1 & {M}_2&0 & {Q}_1{N}_1\T & \hat{{K}}_z\T{N}_2\T & {Q}_1 {N}\T{N}_3\T \\
			* & {\Lambda}_2 & {P}_2{M}_1 & {P}_2{M}_2 & \hat{{L}}_z{M}_3& 0& 0&0\\
			* & * & -2\alpha_1{I} & 0&0&0&0&0\\
			* & * &*  & -2\alpha_2{I}&0&0&0&0\\
			*& * & * &*  &-2\alpha_3{I}&0&0&0\\
			* &* & * & * & *&-2\beta_1{I}&0&0\\
			* & * & * &*& *&*&-2\beta_2{I}& 0\\
			*&*&* &* & * & * & *&-2\beta_3{I}\\
		\end{bmatrix},
	\end{equation}
	%
	\begin{equation}
		\mathcal{T}_{12}= \begin{bmatrix}
			{B}\hat{{K}}_z & 0&0&0&0&0&0&0\\
			0&{I}&0&0&0&0&0&0\\
			0&0& \beta_1{I}&0&0&0&0&0\\
			0&0&0& \beta_2{I}&0&0&0&0\\
			0&0&0&0&\beta_3{I}&0&0&0\\
			0&0&0&0&0& \alpha_1{I}&0&0\\
			{N}_2\hat{{K}}_z&0&0&0&0&0& \alpha_2{I}&0\\
			0&0&0&0&0&0&0&\alpha_3{I}\\
		\end{bmatrix},
	\end{equation}
	%
	\begin{equation}
		\mathcal{T}_{22}=\textnormal{diag}\left(-\frac{1}{\epsilon_1}{Q}_1,-\epsilon_1{Q}_1,{I}\right),
	\end{equation}%
	%
	\begin{align}
		\label{eq:Lambda_1}
		{\Lambda}_1&={Q}_1{A}_n\T+{A}_n{Q}_1+{B}\hat{{K}}_z+\hat{{K}}_z\T{B}\T, \\
		\label{eq:Lambda_2}
		{\Lambda}_2&={A}_n\T{P}_2+{P}_2{A}_n-\hat{{L}}_z{C}{N}-{N}\T{C}\T\hat{{L}}_z\T,
	\end{align}
	%
	и коэффициенты регулятора и наблюдателя находятся следующим образом ${K}_z\ = \hat{{K}}_z{Q}_1^{-1}$ и ${L}_z = {P}_2^{-1} \hat{{L}}_z$.
\end{theorem}
\begin{proof}
	Мы вводим новую переменную $\chi = [z\T \ e_z\T ]\T$ и напишем кандидата в функцию Ляпунова:
	%
	\begin{equation}
		\label{eq:thm3_Lyapunov_candidat}
		V = \begin{bmatrix}
			z  \\ e_z
		\end{bmatrix}\T
		\begin{bmatrix}
			{P}_1 & 0 \\
			0 & {P}_2
		\end{bmatrix}
		\begin{bmatrix}
			z \\
			e_z
		\end{bmatrix}
		=
		\chi\T {P} \chi >0.
	\end{equation}
	%
	Производная от кандидата в функции Ляпунова может быть записана:
	%
	\begin{equation}
		\label{eq:thm3_after_Lyapunov}
		\begin{bmatrix}
			z \\ e_z \\ \eta_1 \\ \eta_2 \\ \eta_3
		\end{bmatrix}\T
		\begin{bmatrix}
			{\Theta}_1 & -{P}_1{B}{K}_z & {P}_1{M}_1 & {P}_1{M}_2 &0 \\
			* &    {\Theta}_2 & {P}_2{M}_1 & {P}_2{M}_2 & {P}_2{L}_z{M}_3\\
			* & * & 0 & 0&0\\
			* & * & * & 0&0 \\
			* & * & * & *&0
		\end{bmatrix}
		\begin{bmatrix}
			z \\ e_z \\ \eta_1 \\ \eta_2 \\ \eta_3
		\end{bmatrix}<0,
	\end{equation}
	%
	где
	%
	\begin{align}
		{\Theta}_1&={P}_1({A}_n+{B}{K}_z)+({A}_n+{B}{K}_z)\T{P}_1 ,\\
		{\Theta}_2&={P}_2({A}_n-{L}_z{CN})+({A}_n-{L}_z{CN})\T{P}_2,
	\end{align}
	%
	и
	%
	\begin{align}
		\eta_1&={F}_1\begin{bmatrix}
			{N}_1 &0
		\end{bmatrix}\chi ,\\
		\eta_2&={F}_2\begin{bmatrix}
			{N}_2{K}_z & -{N}_2{K}_z
		\end{bmatrix}\chi, \\
		\label{eq:eta_3_thm3}
		\eta_3&={F}_3\begin{bmatrix}
			-{N}_3{N} & 0
		\end{bmatrix}\chi.
	\end{align}
	%
	Из ${F}_i\T{F}_i\leq {I}$ следует, что:
	%
	\begin{align}    
		\eta_1\T\eta_1 &\leq \chi\T \begin{bmatrix}
			{N}_1\T \\ 0  
		\end{bmatrix}\begin{bmatrix}
			{N}_1 \ \ 0  
		\end{bmatrix} \chi,
		\\
		\eta_2\T\eta_2 &\leq \chi\T \begin{bmatrix}
			{K}_z\T{N}_2 \\ -{K}_z\T{N}_2\T  
		\end{bmatrix}\begin{bmatrix}
			{N}_2{K}_z & -{N}_2{K}_z
		\end{bmatrix} \chi,
		\\
		\eta_3\T\eta_3 &\leq \chi\T \begin{bmatrix}
			-{N}\T{N}_3\T \\ 0  
		\end{bmatrix}\begin{bmatrix}
			-{N}{N}_3 \ \ 0  
		\end{bmatrix} \chi.
	\end{align}
	
	Используя S-процедуру {\ref{lemma:S_procedure}}, мы получаем условие на положительность кандидата Ляпунова:
	%
	\begin{multline}
		\begin{bmatrix}
			{\Theta}_1 & -{P}_1{B}{K}_z & {P}_1{M}_1 & {P}_1{M}_2 &0 \\
			* &    {\Theta}_2 & {P}_2{M}_1 & {P}_2{M}_2 & {P}_2{L}_z{M}_3\\
			* & * & 0 & 0&0\\
			* & * & * & 0&0 \\
			* & * & * & *&0
		\end{bmatrix} + \\
		\begin{bmatrix}
			\mathcal{X}_{\gamma}& -\gamma_2{K}_z\T{N}_2\T{N}_2{K}_z &0 &0 & 0\\
			*&\gamma_2{K}_z\T{N}_2\T{N}_2{K}_z&0&0&0\\
			*&*&-\gamma_1{I}&0&0\\
			*&*&*&-\gamma_2{I}&0\\
			*&*&*&*&-\gamma_3{I}\\
		\end{bmatrix} 
		<0,
	\end{multline}
	где $\gamma_1 > 0,\gamma_2 > 0$ и $\gamma_3 > 0$, также
	%
	\begin{equation}
		\mathcal{X}_{\gamma}=    \gamma_1{N}_1\T{N}_1 +\gamma_2{K}_z\T{N}_2\T{N}_2{K}_z+\gamma_3{N}\T{N}_3\T{N}_3{N},
	\end{equation}
	Используя дополнение Шура {\ref{lemma:Schur}} и сопрягая блочно-диагональную матрицу $\textbf{diag}({Q}_1, {I})$, где ${Q}_1 = {P}_1^{-1}$, находим:
	%
	\begin{equation}
		\label{eq:thm3_before_Young}
		\begin{bmatrix}
			{\Omega}_1 & -{B}{K}_z & {M}_1 & {M}_2 & 0& {Q}_1{N}_1\T & {Q}_1{K}_z\T{N}_2\T & {Q}_1 {N}\T{N}_3\T 
			\\
			* & {\Theta}_2 & {P}_2{M}_1 & {P}_2{M}_2 & {P}_2{L}_z{M}_3 & 0 & -{K}_z\T{N}_2\T & 0\\
			* & * & -\gamma_1{I} & 0&0&0&0&0\\
			* & * & * & -\gamma_2{I}&0&0&0&0\\
			* & * & * & *&-\gamma_3{I}&0&0&0\\
			* & * & * & *&*&-\frac{1}{\gamma_1}{I}&0&0\\
			* & * & * & *&*&*&-\frac{1}{\gamma_2}{I}&0\\
			*&* & * & * & *&*&*&-\frac{1}{\gamma_3}{I}
		\end{bmatrix}<0,
	\end{equation}
	%
	где
	%
	\begin{equation}
		{\Omega}_1={A}_n{Q}_1+{B}{K}_z{Q}_1+{Q}_1{A}_n\T+{Q}_1{K}_z\T{B}\T.
	\end{equation}
	%
	Используя неравенство Юнга \ref{lemma:Young}, применяя дополнение Шура {\ref{lemma:Schur}} и вводя замену переменных $\hat{{K}}_z={K}_z{Q}_1$, $\hat{{L}}_z={P}_2{L}_z$, получаем:
	%
	\begin{equation}
		\label{eq:thm3_LMI_before_alpha_beta}
		\begin{bmatrix}
			{\Lambda}_1 & 0 & {M}_1 & {M}_2&0 & {Q}_1{N}_1\T & {\Lambda}_3& {\Lambda}_4 & {B}\hat{{K}}_z & 0\\
			* & {\Lambda}_2 & {P}_2{M}_1 & {P}_2{M}_2 & \hat{{L}}_z{M}_3& 0& 0&0&0&{I} \\
			* & * & -\gamma_1{I} & 0&0&0&0&0&0&0\\
			* & * &*  & -\gamma_2{I}&0&0&0&0&0&0\\
			*& * & * &*  & -\gamma_3{I}&0&0&0&0&0\\
			* &* & * & * & *&-\frac{1}{\gamma_1}{I}&0&0&0&0\\
			* & * & * &*& *&*&-\frac{1}{\gamma_2}{I}& 0&{N}_2\hat{{K}}_z&0\\
			*&*&* &* & * & * & *&-\frac{1}{\gamma_3}{I}&0&0\\
			* & * & *&*&* & *&*&*&-\frac{1}{\epsilon_1}{Q}_1&0\\
			* & * & * & *&*&*&*&*&*&-\epsilon_1{Q}_1\\
		\end{bmatrix}<0.
	\end{equation}
	%
	Последний шаг заключается в замене скаляров $\gamma_i$ на пары переменных $\alpha_i$, $\beta_i$, как указано в замечании \ref{rmk:alpha_beta}. После этого условие устойчивости приобретает вид, определённый в теореме \ref{thm:part2_LMI_1}.
\end{proof}
\begin{remark}
	\label{rmk:alpha_beta}
	Можно заменить параметр $\epsilon_3$ новыми переменными $\alpha$ и $\beta$, которые сохраняют линейность ограничения линейного матричного неравенства. Для этого мы предлагаем соотношение $\epsilon_3=\frac{\alpha^2}{\beta^2}$ и требуем положительности новых переменных $\alpha>0$, $\beta>0$. Это приводит к следующим тождествам \cite{KHELOUFI2016}:
	%
	\begin{equation}
		\label{eq:apha_beta_rm_eq1}
		\left(\beta-\frac{\alpha}{\beta}\right)^2 {I} \geq 0 \ \ , \ \  \left(\alpha-\frac{\beta}{\alpha}\right)^2 {I} \geq 0,
	\end{equation}
	%
	\begin{equation}
		\label{eq:apha_beta_rm_eq2}
		-\epsilon_3 {I} = -\frac{\alpha^2}{\beta^2}{I} \leq \beta^2{I}-2\alpha {I} \ \ , \ \  -\frac{1}{\epsilon_3} {I} = -\frac{\beta^2}{\alpha^2}{I} \leq \alpha^2{I}-2\beta {I} .
	\end{equation}
	%
\end{remark}
	%
Это линейное матричное неравенство с переменными ${Q}_1,{P}_2,\hat{{K}}_z$ $\hat{{L}}_z$ , $\alpha_i$, $\beta_i$. Определяя целевую функцию как сумму следов переменных, мы формулируем полуопределённую программу с параметром $\epsilon_1$:
%
\begin{equation}
	\label{eq:thm3_OCP}
	\begin{aligned}
		& \underset{\alpha_i, \beta_i, {Q}_1, {P}_2, \hat{{K}}_z, \hat{{L}}_z}{\text{minimize}}
		& & \operatorname{tr}({Q}_1\T{W}_z{Q}_1)+ \operatorname{tr}({P}_2\T{W}_e{P}_2)+ \operatorname{tr}(\hat{{K}}\T{W}_k\hat{{K}})+\operatorname{tr}(\hat{{L}}\T{W}_l\hat{{L}}), \\
		& \text{subject to}
		& & \begin{cases}
			{Q}_1 > 0, \ \
			{P}_2 > 0, \ \
			\alpha_i >0, \ \
			\beta_i>0 \ \
			\text{for} \ \ i=1,2,3;\\
			\text{condition \eqref{eq:thm3_final_LMI}}.
		\end{cases}
	\end{aligned}
\end{equation}
%
Фиксируя значение параметра, мы находим оптимальный регулятор и наблюдаем усиление, гарантирующее устойчивость системы. Эта оптимизационная задача может быть эффективно решена с помощью решателя полуопределённых программ. Мы можем выполнить сеточный поиск для $\epsilon_1$.
 \begin{remark}
	\label{rm:griding_search}
	Скалярный параметр $\epsilon_1$ выбирается перед решением задачи \eqref{eq:thm3_OCP}. Это приводит к необходимости поиска в диапазоне значений, которые может принимать $\epsilon_1$. Для поиска оптимального $\epsilon_1$ мы используем метод сеток. Пусть $\kappa=\frac{\epsilon_1}{\epsilon_1+1}$, следовательно
	$\epsilon_1=\frac{\kappa}{1-\kappa}$, из чего находим диапазон $\kappa$ как $0 < \kappa < 1$, так как $\epsilon_1 > 0$ \cite{Li1997}. Разделив диапазон $\kappa$ поровну, находим значения $\epsilon_1$, связанные с каждым значением $\kappa$, и решаем задачу \eqref{eq:thm1_OCP} для каждого из них; оптимальным считается тот, который обеспечивает наименьшую стоимость. Этот метод был представлен в \cite{Li1997}.
\end{remark}

\subsection{Мягкая неопределённость}\label{sec:ch3/sect2/sub2}
 В этом подразделе мы рассмотрим случай, когда неопределённости ограничены следующим образом:
 %
 \begin{equation}
 	\label{eq:part2_nu_uncertainty}
 	{F}_i\T{F}_i\leq \nu_i {I}, \ \ i=1,2,3;
 \end{equation}
 %
 Мы можем предложить условия на изменение коэффициента регулятора и наблюдателя, которые гарантируют устойчивость при любом допустимом значении ${F}_1$ для заданного радиуса неопределённости $\nu$:
 \begin{theorem}\label{thm:part2_LMI_2}
 	Система (\ref{eq:part2_system}) асимптотически стабильна для всех матриц
 	$\Delta {A}_n={M}_1{F}_1{N}_1$, 
 	$\Delta {B}= {M}_2{F}_2{N}_2$, 
 	$\Delta {C} = {M}_3{F}_3{N}_3$
 	где
 	${F}_i\T{F}_i\leq \nu_i {I}$ для $i=1,2,3$
 	если существуют такие позитивно-определённые матрицы ${Q}_1$, ${P}_2$и позитивные скаляры
 	$\gamma_1>0$, $\gamma_2>0$, $\gamma_3>0$, $\bar{\mu}_1>0$, $\bar{\mu}_2>0$, $\bar{\mu}_3>0$, и $\epsilon_1 > 0$ такие, чтобы следующие линейные матричные неравенства выполнялись:
 	%
 	\begin{equation}
 		\label{eq:thm4_final_LMI}
 		\begin{bmatrix}
 			{\Lambda}_1 & 0 & {M}_1 & {M}_2&0 & {Q}_1{N}_1\T & {\Lambda}_3 &{\Lambda}_4 & {B}\hat{{K}}_z & 0\\
 			* & {\Lambda}_2 & {P}_2{M}_1 & {P}_2{M}_2 & \hat{{L}}_z{M}_3& 0& 0&0&0&{I} \\
 			* & * & -\gamma_1{I} & 0&0&0&0&0&0&0\\
 			* & * &*  & -\gamma_2{I}&0&0&0&0&0&0\\
 			*& * & * &*  & -\gamma_3{I}&0&0&0&0&0\\
 			* &* & * & * & *&-\bar{\mu}_1{I}&0&0&0&0\\
 			* & * & * &*& *&*&-\bar{\mu}_2{I}& 0&{N}_2\hat{{K}}_z&0\\
 			*&*&* &* & * & * & *&-\bar{\mu}_3{I}&0&0\\
 			* & * & *&*&* & *&*&*&-\frac{1}{\epsilon_1}{Q}_1&0\\
 			* & * & * & *&*&*&*&*&*&-\epsilon_1{Q}_1\\
 		\end{bmatrix}<0,
 	\end{equation}
 	%
 	где
 	%
 	\begin{align}
 		{\Lambda}_3&=\hat{{K}}_z\T{N}_2\T,\\ {\Lambda}_4&={Q}_1{N}\T{N}_3\T,
 	\end{align}
 	%
 	и ${\Lambda}_1$, ${\Lambda}_2$ определены в \eqref{eq:Lambda_1} и \eqref{eq:Lambda_2}. 
 \end{theorem}
 \begin{proof}
 	Доказательство этой теоремы мы начинаем аналогично доказательству теоремы \ref{thm:part2_LMI_1}; шаги, представленные уравнениями \eqref{eq:thm3_Lyapunov_candidat}-\eqref{eq:eta_3_thm3}, те же самые. Отсюда, используя \eqref{eq:part2_nu_uncertainty}, мы получаем:
 	\begin{align}
 		\label{eq:eta1_bound}
 		\eta_1\T\eta_1 &\leq \chi\T \begin{bmatrix}
 			{N}_1\T \\ 0  
 		\end{bmatrix}\nu_1\begin{bmatrix}
 			{N}_1 \ \ 0  
 		\end{bmatrix} \chi,
 		\\
 		\eta_2\T\eta_2 &\leq \chi\T \begin{bmatrix}
 			{K}_z\T{N}_2 \\ -{K}_z\T{N}_2\T  
 		\end{bmatrix}\nu_2\begin{bmatrix}
 			{N}_2{K}_z & -{N}_2{K}_z
 		\end{bmatrix} \chi,
 		\\
 		\eta_3\T\eta_3 &\leq \chi\T \begin{bmatrix}
 			-{N}\T{N}_3\T \\ 0  
 		\end{bmatrix}\nu_3\begin{bmatrix}
 			-{N}{N}_3 \ \ 0  
 		\end{bmatrix} \chi.
 	\end{align}
 	%
 	Используя S-процедуру и определяя $\bar{\mu}_i=\frac{1}{\mu_i}$, напишем:
 	\begin{multline}
 		\begin{bmatrix}
 			{\Theta}_1 & -{P}_1{B}{K}_z & {P}_1{M}_1 & {P}_1{M}_2 &0 \\
 			* &    {\Theta}_2 & {P}_2{M}_1 & {P}_2{M}_2 & {P}_2{L}_z{M}_3\\
 			* & * & 0 & 0&0\\
 			* & * & * & 0&0 \\
 			* & * & * & *&0
 		\end{bmatrix} + \\
 		\begin{bmatrix}
 			\mathcal{X}_{\mu}& -\mu_2{K}_z\T{N}_2\T{N}_2{K}_z &0 &0 & 0\\
 			*&\mu_2{K}_z\T{N}_2\T{N}_2{K}_z&0&0&0\\
 			*&*&-\gamma_1{I}&0&0\\
 			*&*&*&-\gamma_2{I}&0\\
 			*&*&*&*&-\gamma_3{I}\\
 		\end{bmatrix} 
 		<0,
 	\end{multline}
 	где
 	%
 	\begin{equation}
 		\mathcal{X}_{\mu}=\mu_1{N}_1\T{N}_1 +\mu_2{K}_z\T{N}_2\T{N}_2{K}_z+\mu_3{N}\T{N}_3\T{N}_3{N},
 	\end{equation}
 	и $\mu_i=\gamma_i\nu_i$ для $i=1,2,3$, то используя дополнение Шура
 	%
 	\begin{equation}
 		\label{eq:thm4_after_Schur}
 		\begin{bmatrix}
 			{\Theta}_1 & -{P}_1{B}{K}_z & {P}_1{M}_1 & {P}_1{M}_2 & 0 & {N}_1\T & {K}_z\T{N}_2\T & {N}\T{N}_3\T 
 			\\
 			* & {\Theta}_2 & {P}_2{M}_1 & {P}_2{M}_2 & {P}_2{L}_z{M}_3&0& -{K}_z\T{N}_2\T& 0\\
 			* & * & -\gamma_1{I} & 0&0&0&0&0\\
 			* & * & * & -\gamma_2{I}&0&0&0&0\\
 			* & * & * & *&-\gamma_3{I}&0&0&0\\
 			* & * & * & *&*&-\bar{\mu}_1{I}&0&0\\
 			* & * & * & *&*&*&-\bar{\mu}_2{I}&0\\
 			*&* & * & * & *&*&*&-\bar{\mu}_3{I}\\
 		\end{bmatrix}<0.
 	\end{equation}
 	%
 	Продолжая, как и в предыдущих доказательствах, использовать отношение Юнга, дополнение Шура и замену переменных, мы приходим к окончательному линейному матричному неравенству \eqref{eq:thm4_final_LMI}.
 \end{proof}
 В отличие от предыдущих задач оптимизации, здесь не существует естественной формулировки целевой функции. Мы предлагаем два варианта целевой функции:
 \begin{align}
 	\label{eq:cost_lin}
 	J_\text{lin} &= \sum_{i=1}^{3}\left(\bar{\mu}_i+\gamma_i\right) \\ 
 	\label{eq:cost_quad}
 	J_\text{quad} & =  \sum_{i=1}^{3}\left((\bar{\mu}_i-\varpi)^2+(\gamma_i-\varpi)^2\right) + \varpi^2,
 \end{align}
 %
 где $\varpi$ - свободная переменная. Целью обеих функций затрат является максимизация $\nu_i$, которая достигается минимизацией либо $J_\text{lin}$, либо $J_\text{quad}$. Максимизируя $\nu_i$, мы достигаем устойчивости к большему набору неопределённых матриц $\Delta {A}_n$, $\Delta {B}$ и $\Delta {C}$.
 
 Кроме того, может быть интересно ограничить набор неопределённых матриц, ограничив $\nu_i \leq 1$. Утверждение $\nu_i=\frac{1}{\bar{\mu_i}\gamma_i} \leq 1$ преобразуется в $\frac{1}{\bar{\mu}_i}\leq \gamma_i$, и, используя дополнение Шура, мы получаем линейное ограничение: 
  %
 \begin{equation}
 	\label{eq:mu_gamma_limit}
 	\begin{bmatrix}
 		-\gamma_i & 1 \\
 		1 & -\bar{\mu}_i
 	\end{bmatrix}
 	\leq 0. \end{equation}
 %
 Выбирая линейную целевую функцию и ограничивая $\nu_i$, задача оптимизации приобретает вид:
 %
 \begin{equation}
 	\label{eq:thm4_OCP}
 	\begin{aligned}
 		& \underset{\bar{\mu}_i,\gamma_i,{Q}_1, {P}_2,\hat{{K}} , \hat{{L}} }{\text{minimize}}
 		& &  \sum_{i=1}^{3}\left(\bar{\mu}_i+\gamma_i\right), \\
 		& \text{subject to}
 		& & \begin{cases}
 			{Q}_1>0, \ \
 			{P}_2>0, \ \
 			\bar{\mu}_i>0, \ \
 			\gamma_i>0, \ \
 			\text{для} \ \ i=1,2,3; \\
 			\text{условия \eqref{eq:thm4_final_LMI}, \eqref{eq:mu_gamma_limit} }.
 		\end{cases}
 	\end{aligned}
 \end{equation}
 %
 Данная задача оптимизации имеет параметр $\epsilon_1$ который мы можем найти решетчатым поиском как в \ref{rm:griding_search}.
\begin{remark}
	Эту задачу можно рассматривать как обобщение и ослабление исходной задачи, рассмотренной в предыдущем подразделе. Вместо того чтобы искать управление, устойчивое к заданной неопределённости, мы пытаемся найти управление, устойчивое к наибольшей возможной неопределённости.
\end{remark}
%
\section{Аддитивная неопределённость}\label{sec:ch3/sect3}
Рассмотрим следующую систему:
%
\begin{equation}
	\label{eq:part1_linear_dynamics}
	\begin{cases}
		\dot z=({A}_n+\Delta {A}_n)z + {A}_r\zeta + {B}u,\\
		y={C}{N}z+{C}{R}\zeta,
	\end{cases}
\end{equation}
%
где $z$ это динамические состояния, $\zeta = \text{const}$ - статические состояния и $\Delta {A}_n$ представляет мультипликативную модель неопределённостей и имеет следующую структуру:
%
\begin{equation}
	\label{eq:part1_uncertainty}
	\Delta {A}_n={M}_1{F}_1{N}_1 \quad \text{и} \quad {F}_1\T{F}_1\leq \nu {I},
\end{equation}
%
где ${M_1} \in \mathbb{R}^{n_z \times d}$ и 
${N_1} \in \mathbb{R}^{d \times n_z}$ известные матрицы, ${F}_1$ - неизвестная матрица ограниченная по норме и $\nu$ - неизвестный радиус - скаляр, определяющий лимит наложенный на норму ${F}_1$.

Следуя \cite{SAVIN2021} мы можем ввести наблюдатель Люнберга:
%
\begin{equation}
	\begin{bmatrix}
		\dot{\hat{z}} \\
		\dot{\hat{\zeta}}
	\end{bmatrix}=\begin{bmatrix}
		{A}_n & {A}_r \\
		0 & 0
	\end{bmatrix}
	\begin{bmatrix}
		\hat{z}\\ \hat{\zeta}
	\end{bmatrix}
	+  \begin{bmatrix}
		{B}\\0
	\end{bmatrix}u + {L} \left( y-\begin{bmatrix}
		{C}{N} & {C}{R}
	\end{bmatrix} \begin{bmatrix}
		\hat{z}\\ \hat{\zeta}
	\end{bmatrix} \right),
\end{equation}
%
где ${L}$ коэффициент наблюдателя.

Определим ошибку оценки состояния как $e = [ (z-\hat{z})\T \ \ (\zeta-\hat{\zeta})\T ]\T$ и введем блочную матрицу:
${S} = \begin{bmatrix}
	{I} \\ 0
\end{bmatrix}$, 
${E}=[ {N} \ \ {R}]$, и 
$
{A}_c=    \begin{bmatrix}
	{A}_r  & {A}_{\rho} \\
	0  & 0
\end{bmatrix}
$
записываем ошибку наблюдателя динамики:
%
\begin{equation}
	\label{eq:part1_error_dynamics}
	\dot e= ({A}_e-{L}{C}{E})e +{S}\Delta {A}_n z.
\end{equation}
%
Вводим закон управления:
%
\begin{equation}
	u={K}_z \hat{z}+{K}_{\zeta} \hat{\zeta},
\end{equation}
%
и определяем ${K}=\begin{bmatrix}
	{K}_z & {K}_{\zeta}
\end{bmatrix}$ записываем динамическую систему с обратной связью:
%
\begin{equation}
	\label{eq:part1_active_dynamics}
	\dot{z}=({A}_n+\Delta {A}_n +{B}{K}_z)z-{B}{K}e+({A}_r+{B}{K}_{\zeta})\zeta.
\end{equation}
%
Коэффициент регулятора ${K}_{\zeta}$ может быть выбран как:
%
\begin{equation}
	\label{eq:part1_static_control}
	{K}_{\zeta}=-{B}^{\dagger}{A}_r.
\end{equation}
%
Пока столбцы${A}_r$ лежат в подпространстве столбцов ${B}$ и существует точная оценка состояния, данный закон управления сводит на нет эффект $\zeta$ на динамику. В данном случае,отмечая что ${K}_z={K}{S}$,  мы можем представить ошибку наблюдателя и динамику робота как систему уравнений:
%
\begin{equation}
	\label{eq:part1_system}
	\begin{bmatrix}
		\dot{z} \\ \dot{e}
	\end{bmatrix}=\begin{bmatrix}
		({A}_n+\Delta {A}_n +{B}{K}{S}) & {B}{K} \\
		{S} \Delta {A}_n & ({A}_e-{L}{C}{E})        \end{bmatrix}\begin{bmatrix}
		z \\ e
	\end{bmatrix}.
\end{equation}
%
Рассмотрим проблему нахождения таких коэффициентов регулятора и наблюдателя, которые будут давать устойчивую систему для всех допустимых значений $\Delta {A}_n$.

\subsection{Единичная неопределённость}\label{sec:ch3/sect3/sub1}

Начнём с рассмотрения случая, когда неопределённость строго ограничена неравенством ${F}_1\T{F}_1\leq {I}$, которое мы называем единичной неопределённостью. В этом случае следующая теорема даёт нам достаточное условие устойчивости, которое может быть непосредственно использовано при проектировании регуляторов и коэффициентов усиления наблюдателей и представлено в виде линейного матричного неравенства с параметром.
\begin{theorem}\label{thm:part1_LMI_1}
	Система \eqref{eq:part1_system} асимптотически устойчива для всех матриц $\Delta {A}_n={M}_1{F}_1{N}_1$ с ${F}_1\T{F}_1\leq {I}$, если существуют положительно-определённые матрицы ${Q}_1>0$, ${P}_2>0$, матрицы $\hat{{K}}, \hat{{L}}$ и скаляры $\epsilon_1>0,\epsilon_2>0,\epsilon_3>0$ такие, что следующее линейное матричное неравенство выполнимо: 
	%
	\begin{equation}
		\label{eq:thm1_main_LMI}
		\begin{bmatrix}    
			{\Psi}_1  & 0 & {\Xi} & 0 &  {Q}_1{N}_1\T & {Q}_1{N}_1\T & 0\\
			* & {\Psi}_2 & 0 & {I} & 0 & 0 & {P}_2{S}{M}_1\\
			* & * &  -\frac{1}{\epsilon_1}{H} & 0 & 0 &0 & 0\\
			* & * & * & -\epsilon_1{H} & 0 & 0 & 0 \\
			* & * & * & * & -\epsilon_2 {I} & 0 & 0 \\       * & * & * & * & *& -\epsilon_3 {I} & 0 \\
			* & * & * & * & *& * & -\frac{1}{\epsilon_3} {I}
		\end{bmatrix} <0,
	\end{equation}
	%
	где
	%
	\begin{equation}
		\label{eq:H_Xi_variables}
		{H} = \begin{bmatrix}
			{Q}_1 & 0 \\
			0 & {I}
		\end{bmatrix}, \ \ 
		{\Xi} = \begin{bmatrix}
			{B}\hat{{K}} & {B}{K}_{\zeta} \end{bmatrix},
	\end{equation}
	%
	\begin{align}
		\label{eq:Psi_1}
		{\Psi}_1&={Q}_1{A}_n\T+{A}_n{Q}_1+{B}\hat{{K}}+\hat{{K}}\T{B}\T  +\epsilon_2{M}_1{M}_1\T, \\
		\label{eq:Psi_2}
		{\Psi}_2 &={A}_e\T{P}_2+{P}_2{A}_e-\hat{{L}}{CE}-{E}\T{C}\T\hat{{L}}\T,
	\end{align}
	%
	и коэффициенты регулятора и наблюдателя находятся как ${L}={P}^{-1}_2\hat{{L}}$
	и ${KS}=\hat{{K}}{Q}^{-1}_1$.  
\end{theorem}
\begin{proof}
	Введём переменную ${\vartheta}=[
	z\T \ e\T ]\T$ и напишем кандидата в функцию Ляпунова: 
	%
	\begin{equation}
		V
		=
		\begin{bmatrix}
			z \\ e
		\end{bmatrix}\T
		\begin{bmatrix}
			{P}_1 & 0 \\
			0 & {P}_2
		\end{bmatrix}
		\begin{bmatrix}
			z  \\ e
		\end{bmatrix}
		=
		\vartheta\T{P}\vartheta
		>0.
	\end{equation}
	%
	Производная по времени от функции-кандидата Ляпунова представляет собой квадратичную форму, которая должна быть отрицательно-определённой. Это можно записать в виде следующего матричного неравенства:
	\begin{equation}
		\label{eq:matrix_Young}
		\begin{bmatrix}
			{\Sigma}_1+{P}_1\Delta {A}_n+ (\Delta {A}_n)\T {P}_1 & ({S} \Delta {A}_n)\T{P}_2 -{P}_1{BK} \\
			{P}_2{S}\Delta {A}_n - ({BK})\T{P}_1 & {\Sigma}_2
		\end{bmatrix}<0,
	\end{equation}
	%
	где
	%
	\begin{align}
		{\Sigma}_1&= {P}_1({A}_n+{BKS})+({A}_n+{BKS}){P}_1\T, \\  
		{\Sigma}_2&={P}_2({A}_e-{LCE})+({A}_e-{LCE})\T{P}_2.
	\end{align}
	%
	Теперь, подставив \eqref{eq:part1_uncertainty} в \eqref{eq:matrix_Young} и сопрягая по $diag({Q}_1, {I})$, получим:
	\begin{equation}
		\label{eq:Young_conjugated}
		\begin{bmatrix}
			{\Pi}_1+{M}_1{F}_1{N}_1{Q}_1 + {Q}_1({M}_1{F}_1{N}_1)\T   & {Q}_1({S} {M}_1{F}_1{N}_1)\T{P}_2 -{BK} \\
			{P}_2{M}_1{F}_1{N}_1{Q}_1 - ({BK})\T & {\Sigma}_2
		\end{bmatrix} <0,
	\end{equation}
	%
	где $ {\Pi}_1=({A}_n+{BKS}){Q}_1+{Q}_1({A}_n+{BKS})\T$ и ${Q}_1 ={P}_1^{-1}$.
	Используя лемму \ref{lemma:Young} уравнения \eqref{eq:Young_relation_BMI} и \eqref{eq:Young_robust_2} с $\nu=1$ мы расширяем последнее выражение следующим образом (см. приложение \ref{apx:A}):
	\begin{equation}
		\label{eq:thm1_LMI_after_Young}
		\begin{bmatrix}
			{\Gamma}_1  & 0  \\
			0 & {\Sigma}_2 +\frac{1}{\epsilon_1}{H}^{-1} +\epsilon_3 {P}_2{S}{M}_1({P}_2{S}{M}_1)\T
		\end{bmatrix}<0,
	\end{equation}
	%
	где
	%
	\begin{equation}
		\label{eq:thm1_pre_final_LMI_Gamma}
		{\Gamma}_1={\Pi}_1 +\epsilon_1{BKH}{K}\T{B}\T+ \left(\frac{1}{\epsilon_2}+\frac{1}{\epsilon_3} \right){Q}_1{N}_1\T{N}_1{Q}_1 + \epsilon_2 {M}_1{M}_1\T.
	\end{equation}
	%
	Применяя дополнение Шура \ref{lemma:Schur}:
	%
	\begin{equation}
		\label{eq:thm1_pre_final_LMI}
		\begin{bmatrix}
			{\Pi}_1 +\epsilon_2{M}_1{M}_1\T & 0 & {BKH} & 0 &  {Q}_1{N}_1\T & {Q}_1{N}_1\T & 0\\
			* & {\Sigma}_2 & 0 & {I} & 0 & 0 & {P}_2{S}{M}_1\\
			* & * &  -\frac{1}{\epsilon_1}{H} & 0 & 0 &0 & 0\\
			* & * & * & -\epsilon_1{H} & 0 & 0 & 0 \\
			* & * & * & * & -\epsilon_2 {I} & 0 & 0 \\       * & * & * & * & *& -\epsilon_3 {I} & 0 \\
			* & * & * & * & *& * & -\frac{1}{\epsilon_3} {I}\\
		\end{bmatrix} <0.
	\end{equation}
	%
	Замена переменных $\hat{{L}}={P}_2{L}$ и $\hat{{K}}={KS}{Q}_1$
	показывает нам эквивалентности ${\Psi}_1={\Pi}_1+\epsilon_2{M}_1{M}_1\T$ , ${\Psi}_2={\Sigma}_2$ и ${BKH}={\Xi}$; и подставляет их в \eqref{eq:thm1_pre_final_LMI}, находим окончательное линейное матричное неравенство \eqref{eq:thm1_main_LMI}, 
	где ${\Psi}_1$ и ${\Psi}_2$ определяются как выражения \eqref{eq:Psi_1} и \eqref{eq:Psi_2}.
	Это линейное матричное неравенство с переменными ${Q}_1,{P}_2, \hat{{K}},\hat{{L}}$ и $\epsilon_2$. Скаляры $\epsilon_1$ и $\epsilon_3$ постоянные параметры, которые должны быть выбраны перед решением проблемы.
Подставляя \ref{rmk:alpha_beta} в \eqref{eq:thm1_main_LMI} и применяя дополнение Шура, мы получаем линейно матричное неравенство в переменных 
${Q}_1$, ${P}_2$, $\hat{{K}}$, $\hat{{L}}$, $\epsilon_2$, $\alpha$, и $\beta$.
%
\begin{equation}
	\label{eq:thm1_final_LMI_ab}
	\begin{bmatrix}    
		{\Psi}_1  & 0 & {\Xi} & 0 &  {P}_1{N}_1\T & {Q}_1{N}_1\T & 0 & 0 & 0\\
		* & {\Psi}_2 & 0 & {I} & 0 & 0 & {P}_2{S}{M}_1 & 0 & 0\\
		* & * &  -\frac{1}{\epsilon_1}{H} & 0 & 0 &0 & 0& 0 & 0\\
		* & * & * & -\epsilon_1{H} & 0 & 0 & 0 & 0 & 0 \\
		* & * & * & * & -\epsilon_2 {I} & 0 & 0 & 0 & 0 \\       * & * & * & * & *&  -2\alpha {I} & 0 & \beta {I} &0 \\
		* & * & * & * & *& * & -2\beta {I} & 0 & \alpha {I} \\
		* & * & * & * & *&*&* &-{I}&0\\
		* & * & * & * & *&*&*&* &-{I}
	\end{bmatrix} <0.
\end{equation}
\end{proof}

Используя теорему \ref{thm:part1_LMI_1} и добавляя выпуклую целевую функцию, мы формулируем робастную конструкцию управления в виде полуопределённой программы:
%
\begin{equation}
	\label{eq:thm1_OCP}
	\begin{aligned}
		& \underset{\epsilon_2,\alpha,\beta, {Q}_1, {P}_2,\hat{{K}} , \hat{{L}} }{\text{minimize}}
		& & \operatorname{tr}({Q}_1\T{W}_z{Q}_1)+ \operatorname{tr}({P}_2\T{W}_e{P}_2)+ \operatorname{tr}(\hat{{K}}\T{W}_k\hat{{K}})+\operatorname{tr}(\hat{{L}}\T{W}_l\hat{{L}}), \\
		& \text{subject to}
		& & \begin{cases}
			{Q}_1>0, \ \
			{P}_2>0, \ \
			\epsilon_2>0, \ \
			\alpha>0, \ \
			\beta>0, \\
			\text{условие \eqref{eq:thm1_final_LMI_ab} },
		\end{cases}
	\end{aligned}
\end{equation}
где ${W}_z,{W}_e,{W}_k$ и ${W}_l$ - весовые матрицы. 
\subsection{Мягкая неопределённость}\label{sec:ch3/sect3/sub2}
Пусть матрица неопределённости $\Delta {A}_n$ определяется как:
%
\begin{equation}
	\label{eq:nu_uncertainty}
	\Delta {A}_n={M}_1{F}_1{N}_1 \quad \text{и} \quad {F}_1\T{F}_1\leq \nu {I}.
\end{equation}
%
Мы можем предложить условия на усиление регулятора и наблюдателя, которые гарантируют устойчивость при любом допустимом значении ${F}_1$ для заданного радиуса неопределённости $\nu$:
%
\begin{theorem}\label{thm:part1_LMI_2}
	Система \eqref{eq:part1_system}
	асимптотически устойчива для любой $\Delta {A}_n =$${M}_1{F}_1{N}_1$ с ${F}_1\T{F}_1\leq \nu {I}$, если существуют положительно-определённые матрицы ${Q}_1>0$, ${P}_2>0$, матрицы $\hat{{K}}, \hat{{L}}$ и скаляр $\bar{\epsilon}>0$ такие, что следующее линейное матричное неравенство выполнимо: 
	%
	\begin{equation}
		\label{eq:thm2_final_LMI}
		\begin{bmatrix}    
			{\Upsilon}_1  & 0 & {\Xi} & 0 &  {Q}_1{N}_1\T & {M}_1 & 0\\
			* & {\Psi}_2 & 0 & {I} & 0 & 0 & {P}_2{S}{M}_1\\
			* & * &  -\frac{1}{\epsilon_1}{H} & 0 & 0 &0 & 0\\
			* & * & * & -\epsilon_1{H} & 0 & 0 & 0 \\
			* & * & * & * & -\frac{\bar{\epsilon}}{2}{I} & 0 & 0 \\       * & * & * & * & *&  -\bar{\epsilon}{I} & 0 \\
			* & * & * & * & *& * &  -\bar{\epsilon}{I}
		\end{bmatrix} <0,
	\end{equation}
	%
	где
	%
	\begin{equation}
		\label{eq:Upsilon_1}
		{\Upsilon}_1={Q}_1{A}_n\T+{A}_n{Q}_1+{B}\hat{{K}}+\hat{{K}}\T{B}\T, 
	\end{equation}
	Матрицы ${\Psi}_2$, ${\Xi}$ те же, что и в уравнениях \eqref{eq:Psi_2},\eqref{eq:H_Xi_variables},
	а коэффициенты регулятора и наблюдателя задаются ${L}={P}^{-1}_2\hat{{L}}$.
	и ${KS}=\hat{{K}}{Q}^{-1}_1$.
\end{theorem}
\begin{proof}
	Первая часть доказательства совпадает с доказательством теоремы \ref{thm:part1_LMI_1} (до уравнения \eqref{eq:Young_conjugated}). Используя уравнения \eqref{eq:Young_relation_BMI} и \eqref{eq:updated_Young_robust_2} из леммы \ref{lemma:Young}, получаем
	%
	\begin{equation}
		\label{eq:thm2_LMI_after_Young}
		\begin{bmatrix}
			{\Phi}  & 0  \\
			0 & {\Sigma}_2 +\frac{1}{\epsilon_1}{H}^{-1} +\epsilon {P}_2{S}{M}_1({P}_2{S}{M}_1)\T
		\end{bmatrix}<0,
	\end{equation}
	%
	где $\epsilon=\sqrt{\nu}>0$ и
	%
	\begin{equation}
		{\Phi}={\Sigma}_1 
		+\epsilon_1{BKH}{K}\T{B}\T+ 2\epsilon {Q}_1{N}_1\T{N}_1{Q}_1 + \epsilon {M}_1{M}_1\T.
	\end{equation}
	%
	Используя дополнение Шура, преобразуем это условие в: 
	%
	\begin{equation}
		\label{eq:thm2_pre_final_LMI}
		\begin{bmatrix}
			{\Sigma}_1  & 0 & {BKH} & 0 &  {Q}_1{N}_1\T & {M}_1 & 0\\
			* & {\Sigma}_2 & 0 & {I} & 0 & 0 & {P}_2{S}{M}_1\\
			* & * &  -\frac{1}{\epsilon_1}{H} & 0 & 0 &0 & 0\\
			* & * & * & -\epsilon_1{H} & 0 & 0 & 0 \\
			* & * & * & * & -\frac{\bar{\epsilon}}{2}{I} & 0 & 0 \\       * & * & * & * & *& -\bar{\epsilon} {I} & 0 \\
			* & * & * & * & *& * & -\bar{\epsilon} {I}\\
		\end{bmatrix} <0,
	\end{equation}
	%
	где $\bar{\epsilon}=\frac{1}{\epsilon}$, и, наконец, используя замену переменных 
	$\hat{{L}}={P}_2{L}$ и $\hat{{K}}={KS}{Q}_1$
	находим, что ${\Upsilon}_1={\Sigma}_1$, ${\Psi}_2={\Sigma}_2$ и ${BKH}={\Xi}$ и, подставляя в \eqref{eq:thm2_pre_final_LMI}, находим окончательное линейное матричное неравенство \eqref{eq:thm2_final_LMI}. 
\end{proof}
Это неравенство в переменных ${Q}_1,{P}_2,\hat{{K}} , \hat{{L}},\bar{\epsilon}$.
Мы можем выполнить поиск $\epsilon_1$, как описано в примечании \ref{rm:griding_search}. 

В качестве функции стоимости мы можем использовать минимизацию $\bar{\epsilon}$, что эквивалентно максимизации $\nu$, так как $\nu=\frac{1}{\bar{\epsilon}^2}$. Тогда задача оптимизации приобретает вид:
%
\begin{equation}
	\label{eq:thm2_OCP}
	\begin{aligned}
		& \underset{\bar{\epsilon},{Q}_1, {P}_2,\hat{{K}} , \hat{{L}} }{\text{minimize}}
		& &  \bar{\epsilon}, \\
		& \text{subject to}
		& & \begin{cases}
			{Q}_1>0, \ \
			{P}_2>0, \ \
			\bar{\epsilon}>0, \\
			\text{condition \eqref{eq:thm2_final_LMI} }.
		\end{cases}
	\end{aligned}
\end{equation}
\begin{remark}
	\label{rm:nu_trick}
	Добавление условия $\bar{\epsilon}\geq 1$ к \eqref{eq:thm2_OCP} эквивалентно ограничению $\nu \leq 1$. Это полезно, если мы заинтересованы в решении задачи ${F}_1\T{F}_1 \leq {I}$, но эта задача невыполнима, и мы хотели бы знать наибольший радиус $\nu$, для которого задача может быть решена. Таким образом, двоичный вопрос «да или нет» о выполнимости исходной задачи сводится к непрерывному вопросу «насколько велика неопределённость, которую мы можем допустить».
\end{remark}
\clearpage
